{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c57393c9",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593e69e1",
   "metadata": {},
   "source": [
    "3 different subcorpora, belonging to 3 different genres (aka registers or text types). Each genre at least 5,000 words long \n",
    "\n",
    "1. The length (in words).\n",
    "2. The lexical diversity.\n",
    "3. Top 10 most frequent words and their counts.\n",
    "4. Words that are at least 10 characters long and their counts.\n",
    "5. The longest sentence (type the sentence and give the number of words). Hint: look at the Gutenberg part of Section 2.1 in NLTK.\n",
    "6. A stemmed version of the longest sentence.\n",
    "\n",
    "7. Overall (not for each subcorpus): A reflection (1 paragraph or so): What do the most frequent words, the longest words, and longest sentence tell you about each of the 3 genres? How do you interpret the lexical diversity? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95c83d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#put any imports\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6109088d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_diversity(text):\n",
    "    return len(set(text)) / len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "450cea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequent_words(text):\n",
    "    fdist = FreqDist(text)\n",
    "    return fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b104f55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_words(text, word_list, count_list):\n",
    "    for w in sorted(set(w.lower() for w in text if w.isalpha())): #should we normalize? or just do sorted(set(HuckTockens)\n",
    "        if len(w) > 10:\n",
    "            word_list.append(w)\n",
    "            count_list.append(text.count(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "545e5472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences(path, text_file):\n",
    "    subCorpus = PlaintextCorpusReader(path, text_file)\n",
    "    subCorpora1_sentences = subCorpus.sents() #puts sentences into list: [[sentence 1], [sentence 2], etc.]\n",
    "\n",
    "    longest_len = max(len(s) for s in subCorpora1_sentences) #max length sentence\n",
    "    longest_sentence = \"\" #empty variable to store longest sentence\n",
    "    \n",
    "    #find sentence with longest_len\n",
    "    for s in subCorpora1_sentences:\n",
    "        if len(s) == longest_len:\n",
    "            longest_sentence = s\n",
    "    return longest_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "13f25573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmed_sentence(sentence):\n",
    "    ps = PorterStemmer()\n",
    "    stemmed_sentence = []\n",
    "    \n",
    "    #i is the index at which the word is in the list \n",
    "    for i in range(len(sentence)):\n",
    "        #modifies the stemmed_sentence list to contain the stemmed version of each word\n",
    "        stemmed_sentence.append(ps.stem(sentence[i]))\n",
    "    return stemmed_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "20bbd8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubCorpus 1: Research Paper\n",
      "1. Length (in words): 9395 \n",
      "\n",
      "2. Lexical diversity: 0.25502927088877064 \n",
      "\n",
      "3. Top 10 most frequent words and their counts:\n",
      " [(',', 658), ('the', 387), ('.', 375), ('of', 244), ('and', 239), ('a', 188), ('’', 159), ('to', 151), ('in', 139), ('“', 138)] \n",
      "\n",
      "4. Words that are at least 10 characters long and their counts:\n",
      "abandonment 1\n",
      "accelerating 1\n",
      "accomplishment 1\n",
      "acquisition 1\n",
      "acquisitive 1\n",
      "adjudication 1\n",
      "alternatives 1\n",
      "ambiguously 1\n",
      "anthropocentric 0\n",
      "anticipates 2\n",
      "antithetical 2\n",
      "apocalyptic 3\n",
      "appropriate 1\n",
      "architecture 1\n",
      "aspirations 1\n",
      "association 0\n",
      "bestselling 1\n",
      "celebrating 1\n",
      "characterized 1\n",
      "checkerslike 1\n",
      "chronological 1\n",
      "civilization 1\n",
      "commentators 1\n",
      "communicated 1\n",
      "communication 2\n",
      "comparative 0\n",
      "competition 1\n",
      "complicating 1\n",
      "complication 1\n",
      "computerization 1\n",
      "concentrated 1\n",
      "concentration 1\n",
      "conditioning 1\n",
      "considering 0\n",
      "constitutive 1\n",
      "constraints 2\n",
      "constructing 1\n",
      "construction 1\n",
      "contemporary 2\n",
      "contradictions 1\n",
      "contradictory 2\n",
      "contraption 1\n",
      "contrasting 1\n",
      "contributed 1\n",
      "conventional 4\n",
      "conventions 1\n",
      "cooperation 2\n",
      "cultivation 1\n",
      "demonstrate 3\n",
      "demonstrated 1\n",
      "demonstrates 2\n",
      "demonstrating 1\n",
      "demonstration 2\n",
      "description 4\n",
      "despondency 1\n",
      "destructive 1\n",
      "deterioration 2\n",
      "development 1\n",
      "diametrically 1\n",
      "diminishing 1\n",
      "discontents 0\n",
      "disenchantment 1\n",
      "disillusioned 1\n",
      "distractions 1\n",
      "dramatization 1\n",
      "dystopianism 1\n",
      "emphatically 1\n",
      "encroachment 1\n",
      "endorsements 1\n",
      "environment 9\n",
      "environmental 3\n",
      "essentially 2\n",
      "estrangement 1\n",
      "euphorically 1\n",
      "examination 1\n",
      "expectations 1\n",
      "experienced 1\n",
      "experiences 1\n",
      "experiencing 1\n",
      "explanation 2\n",
      "exploration 1\n",
      "facilitates 1\n",
      "foreshadowing 1\n",
      "formulating 1\n",
      "foundational 1\n",
      "friendships 0\n",
      "fundamentally 1\n",
      "generations 1\n",
      "globalization 1\n",
      "grasshopper 15\n",
      "grasshoppers 1\n",
      "hierarchies 1\n",
      "identifiable 2\n",
      "illustrates 1\n",
      "imagination 1\n",
      "imaginative 1\n",
      "implication 1\n",
      "impoverished 1\n",
      "increasingly 1\n",
      "indifference 2\n",
      "indifferent 1\n",
      "individuals 1\n",
      "inefficient 1\n",
      "influential 1\n",
      "instruction 1\n",
      "interaction 5\n",
      "interpersonal 3\n",
      "interstellar 2\n",
      "intervening 1\n",
      "intimations 1\n",
      "intimidation 1\n",
      "introducing 1\n",
      "involvement 1\n",
      "irrevocably 1\n",
      "juxtaposition 1\n",
      "legislation 1\n",
      "limitations 1\n",
      "malleability 1\n",
      "meticulously 1\n",
      "misunderstand 1\n",
      "multiplayer 3\n",
      "multiplicity 1\n",
      "nevertheless 0\n",
      "obliterates 1\n",
      "observation 2\n",
      "obsessively 1\n",
      "occasionally 1\n",
      "ontologically 0\n",
      "overcrowded 1\n",
      "overlapping 4\n",
      "paradoxical 4\n",
      "participants 1\n",
      "particularly 4\n",
      "perceptions 1\n",
      "periodicals 1\n",
      "perspective 6\n",
      "perspectives 2\n",
      "peterborough 0\n",
      "philosopher 1\n",
      "philosophical 5\n",
      "playfulness 2\n",
      "polemicizing 1\n",
      "politicians 1\n",
      "possibilities 2\n",
      "possibility 1\n",
      "presentation 1\n",
      "programmers 1\n",
      "programming 2\n",
      "proliferation 1\n",
      "protagonist 3\n",
      "prototypical 1\n",
      "publication 0\n",
      "realization 1\n",
      "rearranging 1\n",
      "recognizing 1\n",
      "reflections 0\n",
      "relationships 5\n",
      "representation 1\n",
      "reservations 1\n",
      "satisfaction 1\n",
      "sequentially 1\n",
      "significant 3\n",
      "simplification 1\n",
      "simulations 1\n",
      "simultaneously 3\n",
      "sophisticated 1\n",
      "specifically 2\n",
      "spontaneous 1\n",
      "stockholder 1\n",
      "storytelling 1\n",
      "straightforward 2\n",
      "summarizing 0\n",
      "supplements 1\n",
      "surroundings 1\n",
      "technologically 1\n",
      "theorization 1\n",
      "threatening 1\n",
      "timelessness 1\n",
      "traditional 2\n",
      "transformed 2\n",
      "undefinable 1\n",
      "underhanded 1\n",
      "underscores 2\n",
      "understanding 7\n",
      "unnecessary 3\n",
      "wakefulness 1\n",
      "watercutter 0\n",
      "wittgenstein 0\n",
      "\n",
      "\n",
      "The longest sentence:\n",
      " ['The', 'impetus', 'for', 'this', 'endeavor', ',', 'Suits', 'states', 'in', 'his', 'preface', ',', 'is', 'the', '“', 'widespread', 'disenchantment', 'with', 'the', 'search', 'for', 'defini', '-', 'tions', 'that', 'currently', 'prevails', 'in', 'the', 'philosophical', 'community', '”', 'as', 'evidenced', 'by', 'Ludwig', 'Wittgenstein', '’', 's', 'observation', 'that', 'games', 'are', 'undefinable', ',', 'and', ',', 'by', 'extension', ',', 'that', 'the', 'search', 'for', 'any', 'definition', 'is', 'ultimately', 'hopeless', '.', '22', 'Not', 'so', ',', 'argues', 'Suits', ',', 'who', 'examines', 'many', 'facets', 'of', 'gameplay', 'before', 'arriving', 'at', 'a', 'fairly', 'long', 'definition', ':', '“', 'To', 'play', 'a', 'game', 'is', 'to', 'engage', 'in', 'activity', 'directed', 'towards', 'bringing', 'about', 'a', 'specific', 'state', 'of', 'affairs', ',', 'using', 'only', 'means', 'permitted', 'by', 'rules', ',', 'where', 'rules', 'prohibit', 'more', 'efficient', 'in', 'favour', 'of', 'less', 'effi', '-', 'cient', 'means', '.”', 'Or', ',', 'to', 'put', 'it', 'simply', ':', '“', 'Playing', 'a', 'game', 'is', 'the', 'voluntary', 'attempt', 'to', 'overcome', 'unnecessary', 'obstacles', '.”', '23', 'One', 'implication', 'of', 'Suits', '’', 'definition', ',', 'ironically', ',', 'is', 'that', 'it', 'means', 'he', 'is', 'playing', 'a', 'game', 'in', 'the', 'writing', 'of', 'this', 'book', 'itself', '.']\n",
      "The length of the longest sentence: 169 \n",
      "\n",
      "The stemmed longest sentence:\n",
      " ['the', 'impetu', 'for', 'thi', 'endeavor', ',', 'suit', 'state', 'in', 'hi', 'prefac', ',', 'is', 'the', '“', 'widespread', 'disenchant', 'with', 'the', 'search', 'for', 'defini', '-', 'tion', 'that', 'current', 'prevail', 'in', 'the', 'philosoph', 'commun', '”', 'as', 'evidenc', 'by', 'ludwig', 'wittgenstein', '’', 's', 'observ', 'that', 'game', 'are', 'undefin', ',', 'and', ',', 'by', 'extens', ',', 'that', 'the', 'search', 'for', 'ani', 'definit', 'is', 'ultim', 'hopeless', '.', '22', 'not', 'so', ',', 'argu', 'suit', ',', 'who', 'examin', 'mani', 'facet', 'of', 'gameplay', 'befor', 'arriv', 'at', 'a', 'fairli', 'long', 'definit', ':', '“', 'to', 'play', 'a', 'game', 'is', 'to', 'engag', 'in', 'activ', 'direct', 'toward', 'bring', 'about', 'a', 'specif', 'state', 'of', 'affair', ',', 'use', 'onli', 'mean', 'permit', 'by', 'rule', ',', 'where', 'rule', 'prohibit', 'more', 'effici', 'in', 'favour', 'of', 'less', 'effi', '-', 'cient', 'mean', '.”', 'or', ',', 'to', 'put', 'it', 'simpli', ':', '“', 'play', 'a', 'game', 'is', 'the', 'voluntari', 'attempt', 'to', 'overcom', 'unnecessari', 'obstacl', '.”', '23', 'one', 'implic', 'of', 'suit', '’', 'definit', ',', 'iron', ',', 'is', 'that', 'it', 'mean', 'he', 'is', 'play', 'a', 'game', 'in', 'the', 'write', 'of', 'thi', 'book', 'itself', '.']\n",
      "The length of the stemmed longest sentence: 169\n"
     ]
    }
   ],
   "source": [
    "#subcorpora 1\n",
    "with open(\"./dataset/researchPaper.txt\", \"r\", encoding = \"utf8\") as f:\n",
    "    corpora = f.read()\n",
    "subCorpora1 = nltk.word_tokenize(corpora)\n",
    "\n",
    "#top 10 most frequent words & their counts\n",
    "fdist1 = frequent_words(subCorpora1)\n",
    "\n",
    "#words at least 10 characters long & their counts\n",
    "words_list = []\n",
    "count_list = []\n",
    "long_words(subCorpora1, words_list, count_list)\n",
    "  \n",
    "#longest sentence with number of words\n",
    "path = \"./dataset/\"\n",
    "text_file = \"researchPaper.txt\"\n",
    "longest_sentence = sentences(path, text_file)\n",
    "\n",
    "#stemmed version of the longest sentence\n",
    "stemmed_longest_sentence = stemmed_sentence(longest_sentence)\n",
    "\n",
    "\n",
    "print(\"SubCorpus 1: Research Paper\")\n",
    "print(\"1. Length (in words):\", len(subCorpora1), \"\\n\")\n",
    "print(\"2. Lexical diversity:\", lexical_diversity(subCorpora1), \"\\n\")\n",
    "print(\"3. Top 10 most frequent words and their counts:\\n\", fdist1.most_common(10), \"\\n\")\n",
    "print(\"4. Words that are at least 10 characters long and their counts:\")\n",
    "for i in range(len(words_list)):\n",
    "    print(words_list[i], count_list[i])\n",
    "print(\"\\n\")\n",
    "print(\"The longest sentence:\\n\", longest_sentence)\n",
    "print(\"The length of the longest sentence:\", len(longest_sentence), \"\\n\")\n",
    "print(\"The stemmed longest sentence:\\n\", stemmed_longest_sentence)\n",
    "print(\"The length of the stemmed longest sentence:\", len(longest_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174a70cc",
   "metadata": {},
   "source": [
    "need to repeat the above 2 more times; for each subcorpus"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

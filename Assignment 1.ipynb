{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5804ba30",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb50cb98",
   "metadata": {},
   "source": [
    "3 different subcorpora, belonging to 3 different genres (aka registers or text types). Each genre at least 5,000 words long \n",
    "\n",
    "1. The length (in words).\n",
    "2. The lexical diversity.\n",
    "3. Top 10 most frequent words and their counts.\n",
    "4. Words that are at least 10 characters long and their counts.\n",
    "5. The longest sentence (type the sentence and give the number of words). Hint: look at the Gutenberg part of Section 2.1 in NLTK.\n",
    "6. A stemmed version of the longest sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ead9c7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#put any imports\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "85bed9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_diversity(text):\n",
    "    return len(set(text)) / len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c89f7863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequent_words(text):\n",
    "    # Filter out punctuation, single quotes, and double quotes from the tokenized words\n",
    "    words = [word for word in text if word not in string.punctuation and word != \"'\" and word != '”' and word !=\"’\" and word !='“']\n",
    "    \n",
    "    # Create frequency distribution\n",
    "    fdist = FreqDist(words)\n",
    "    return fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e9c2e3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_words(text, word_list, count_list):\n",
    "    word_list.clear() #make sure nothing is in the list before appending\n",
    "    count_list.clear() #make sure nothing is in the list before appending\n",
    "    for w in sorted(set(w.lower() for w in text if w.isalpha())): #normalize to get rid of duplicates\n",
    "        if len(w) > 10:\n",
    "            word_list.append(w)\n",
    "            count_list.append(text.count(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a0804914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences(path, text_file):\n",
    "    subCorpus = PlaintextCorpusReader(path, text_file)\n",
    "    subCorpora1_sentences = subCorpus.sents() #puts sentences into list: [[sentence 1], [sentence 2], etc.]\n",
    "\n",
    "    longest_len = max(len(s) for s in subCorpora1_sentences) #max length sentence\n",
    "    longest_sentence = \"\" #empty variable to store longest sentence\n",
    "    \n",
    "    #find sentence with longest_len\n",
    "    for s in subCorpora1_sentences:\n",
    "        if len(s) == longest_len:\n",
    "            longest_sentence = s\n",
    "    return longest_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7eb0dd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmed_sentence(sentence):\n",
    "    ps = PorterStemmer()\n",
    "    stemmed_sentence = []\n",
    "    \n",
    "    #i is the index at which the word is in the list \n",
    "    for i in range(len(sentence)):\n",
    "        #modifies the stemmed_sentence list to contain the stemmed version of each word\n",
    "        stemmed_sentence.append(ps.stem(sentence[i]))\n",
    "    return stemmed_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9c4c2b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_answers(title, subCorpora, fdist, words_list, count_list, longest_sent, stemmed_sent):\n",
    "    print(\"SubCorpus:\", title)\n",
    "    print(\"1. Length (in words):\", len(subCorpora), \"\\n\")\n",
    "    print(\"2. Lexical diversity:\", lexical_diversity(subCorpora), \"\\n\")\n",
    "    print(\"3. Top 10 most frequent words and their counts:\\n\", fdist.most_common(10), \"\\n\")\n",
    "    print(\"4. Words that are at least 10 characters long and their counts:\")\n",
    "    for i in range(len(words_list)):\n",
    "        print(words_list[i], count_list[i])\n",
    "    print(\"\\n\")\n",
    "    print(\"The longest sentence:\\n\", longest_sent)\n",
    "    print(\"The length of the longest sentence:\", len(longest_sent), \"\\n\")\n",
    "    print(\"The stemmed longest sentence:\\n\", stemmed_sent)\n",
    "    print(\"The length of the stemmed longest sentence:\", len(stemmed_sent), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5c4dfd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./dataset/\"\n",
    "words_list = []\n",
    "count_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d1fb47",
   "metadata": {},
   "source": [
    "### subcorpus 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "56621dc3-0525-452d-8909-73742498e563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubCorpus: Research Paper\n",
      "1. Length (in words): 9395 \n",
      "\n",
      "2. Lexical diversity: 0.25502927088877064 \n",
      "\n",
      "3. Top 10 most frequent words and their counts:\n",
      " [('the', 387), ('of', 244), ('and', 239), ('a', 188), ('to', 151), ('in', 139), ('s', 103), ('is', 86), ('that', 78), ('as', 74)] \n",
      "\n",
      "4. Words that are at least 10 characters long and their counts:\n",
      "abandonment 1\n",
      "accelerating 1\n",
      "accomplishment 1\n",
      "acquisition 1\n",
      "acquisitive 1\n",
      "adjudication 1\n",
      "alternatives 1\n",
      "ambiguously 1\n",
      "anthropocentric 0\n",
      "anticipates 2\n",
      "antithetical 2\n",
      "apocalyptic 3\n",
      "appropriate 1\n",
      "architecture 1\n",
      "aspirations 1\n",
      "association 0\n",
      "bestselling 1\n",
      "celebrating 1\n",
      "characterized 1\n",
      "checkerslike 1\n",
      "chronological 1\n",
      "civilization 1\n",
      "commentators 1\n",
      "communicated 1\n",
      "communication 2\n",
      "comparative 0\n",
      "competition 1\n",
      "complicating 1\n",
      "complication 1\n",
      "computerization 1\n",
      "concentrated 1\n",
      "concentration 1\n",
      "conditioning 1\n",
      "considering 0\n",
      "constitutive 1\n",
      "constraints 2\n",
      "constructing 1\n",
      "construction 1\n",
      "contemporary 2\n",
      "contradictions 1\n",
      "contradictory 2\n",
      "contraption 1\n",
      "contrasting 1\n",
      "contributed 1\n",
      "conventional 4\n",
      "conventions 1\n",
      "cooperation 2\n",
      "cultivation 1\n",
      "demonstrate 3\n",
      "demonstrated 1\n",
      "demonstrates 2\n",
      "demonstrating 1\n",
      "demonstration 2\n",
      "description 4\n",
      "despondency 1\n",
      "destructive 1\n",
      "deterioration 2\n",
      "development 1\n",
      "diametrically 1\n",
      "diminishing 1\n",
      "discontents 0\n",
      "disenchantment 1\n",
      "disillusioned 1\n",
      "distractions 1\n",
      "dramatization 1\n",
      "dystopianism 1\n",
      "emphatically 1\n",
      "encroachment 1\n",
      "endorsements 1\n",
      "environment 9\n",
      "environmental 3\n",
      "essentially 2\n",
      "estrangement 1\n",
      "euphorically 1\n",
      "examination 1\n",
      "expectations 1\n",
      "experienced 1\n",
      "experiences 1\n",
      "experiencing 1\n",
      "explanation 2\n",
      "exploration 1\n",
      "facilitates 1\n",
      "foreshadowing 1\n",
      "formulating 1\n",
      "foundational 1\n",
      "friendships 0\n",
      "fundamentally 1\n",
      "generations 1\n",
      "globalization 1\n",
      "grasshopper 15\n",
      "grasshoppers 1\n",
      "hierarchies 1\n",
      "identifiable 2\n",
      "illustrates 1\n",
      "imagination 1\n",
      "imaginative 1\n",
      "implication 1\n",
      "impoverished 1\n",
      "increasingly 1\n",
      "indifference 2\n",
      "indifferent 1\n",
      "individuals 1\n",
      "inefficient 1\n",
      "influential 1\n",
      "instruction 1\n",
      "interaction 5\n",
      "interpersonal 3\n",
      "interstellar 2\n",
      "intervening 1\n",
      "intimations 1\n",
      "intimidation 1\n",
      "introducing 1\n",
      "involvement 1\n",
      "irrevocably 1\n",
      "juxtaposition 1\n",
      "legislation 1\n",
      "limitations 1\n",
      "malleability 1\n",
      "meticulously 1\n",
      "misunderstand 1\n",
      "multiplayer 3\n",
      "multiplicity 1\n",
      "nevertheless 0\n",
      "obliterates 1\n",
      "observation 2\n",
      "obsessively 1\n",
      "occasionally 1\n",
      "ontologically 0\n",
      "overcrowded 1\n",
      "overlapping 4\n",
      "paradoxical 4\n",
      "participants 1\n",
      "particularly 4\n",
      "perceptions 1\n",
      "periodicals 1\n",
      "perspective 6\n",
      "perspectives 2\n",
      "peterborough 0\n",
      "philosopher 1\n",
      "philosophical 5\n",
      "playfulness 2\n",
      "polemicizing 1\n",
      "politicians 1\n",
      "possibilities 2\n",
      "possibility 1\n",
      "presentation 1\n",
      "programmers 1\n",
      "programming 2\n",
      "proliferation 1\n",
      "protagonist 3\n",
      "prototypical 1\n",
      "publication 0\n",
      "realization 1\n",
      "rearranging 1\n",
      "recognizing 1\n",
      "reflections 0\n",
      "relationships 5\n",
      "representation 1\n",
      "reservations 1\n",
      "satisfaction 1\n",
      "sequentially 1\n",
      "significant 3\n",
      "simplification 1\n",
      "simulations 1\n",
      "simultaneously 3\n",
      "sophisticated 1\n",
      "specifically 2\n",
      "spontaneous 1\n",
      "stockholder 1\n",
      "storytelling 1\n",
      "straightforward 2\n",
      "summarizing 0\n",
      "supplements 1\n",
      "surroundings 1\n",
      "technologically 1\n",
      "theorization 1\n",
      "threatening 1\n",
      "timelessness 1\n",
      "traditional 2\n",
      "transformed 2\n",
      "undefinable 1\n",
      "underhanded 1\n",
      "underscores 2\n",
      "understanding 7\n",
      "unnecessary 3\n",
      "wakefulness 1\n",
      "watercutter 0\n",
      "wittgenstein 0\n",
      "\n",
      "\n",
      "The longest sentence:\n",
      " ['The', 'impetus', 'for', 'this', 'endeavor', ',', 'Suits', 'states', 'in', 'his', 'preface', ',', 'is', 'the', '“', 'widespread', 'disenchantment', 'with', 'the', 'search', 'for', 'defini', '-', 'tions', 'that', 'currently', 'prevails', 'in', 'the', 'philosophical', 'community', '”', 'as', 'evidenced', 'by', 'Ludwig', 'Wittgenstein', '’', 's', 'observation', 'that', 'games', 'are', 'undefinable', ',', 'and', ',', 'by', 'extension', ',', 'that', 'the', 'search', 'for', 'any', 'definition', 'is', 'ultimately', 'hopeless', '.', '22', 'Not', 'so', ',', 'argues', 'Suits', ',', 'who', 'examines', 'many', 'facets', 'of', 'gameplay', 'before', 'arriving', 'at', 'a', 'fairly', 'long', 'definition', ':', '“', 'To', 'play', 'a', 'game', 'is', 'to', 'engage', 'in', 'activity', 'directed', 'towards', 'bringing', 'about', 'a', 'specific', 'state', 'of', 'affairs', ',', 'using', 'only', 'means', 'permitted', 'by', 'rules', ',', 'where', 'rules', 'prohibit', 'more', 'efficient', 'in', 'favour', 'of', 'less', 'effi', '-', 'cient', 'means', '.”', 'Or', ',', 'to', 'put', 'it', 'simply', ':', '“', 'Playing', 'a', 'game', 'is', 'the', 'voluntary', 'attempt', 'to', 'overcome', 'unnecessary', 'obstacles', '.”', '23', 'One', 'implication', 'of', 'Suits', '’', 'definition', ',', 'ironically', ',', 'is', 'that', 'it', 'means', 'he', 'is', 'playing', 'a', 'game', 'in', 'the', 'writing', 'of', 'this', 'book', 'itself', '.']\n",
      "The length of the longest sentence: 169 \n",
      "\n",
      "The stemmed longest sentence:\n",
      " ['the', 'impetu', 'for', 'thi', 'endeavor', ',', 'suit', 'state', 'in', 'hi', 'prefac', ',', 'is', 'the', '“', 'widespread', 'disenchant', 'with', 'the', 'search', 'for', 'defini', '-', 'tion', 'that', 'current', 'prevail', 'in', 'the', 'philosoph', 'commun', '”', 'as', 'evidenc', 'by', 'ludwig', 'wittgenstein', '’', 's', 'observ', 'that', 'game', 'are', 'undefin', ',', 'and', ',', 'by', 'extens', ',', 'that', 'the', 'search', 'for', 'ani', 'definit', 'is', 'ultim', 'hopeless', '.', '22', 'not', 'so', ',', 'argu', 'suit', ',', 'who', 'examin', 'mani', 'facet', 'of', 'gameplay', 'befor', 'arriv', 'at', 'a', 'fairli', 'long', 'definit', ':', '“', 'to', 'play', 'a', 'game', 'is', 'to', 'engag', 'in', 'activ', 'direct', 'toward', 'bring', 'about', 'a', 'specif', 'state', 'of', 'affair', ',', 'use', 'onli', 'mean', 'permit', 'by', 'rule', ',', 'where', 'rule', 'prohibit', 'more', 'effici', 'in', 'favour', 'of', 'less', 'effi', '-', 'cient', 'mean', '.”', 'or', ',', 'to', 'put', 'it', 'simpli', ':', '“', 'play', 'a', 'game', 'is', 'the', 'voluntari', 'attempt', 'to', 'overcom', 'unnecessari', 'obstacl', '.”', '23', 'one', 'implic', 'of', 'suit', '’', 'definit', ',', 'iron', ',', 'is', 'that', 'it', 'mean', 'he', 'is', 'play', 'a', 'game', 'in', 'the', 'write', 'of', 'thi', 'book', 'itself', '.']\n",
      "The length of the stemmed longest sentence: 169 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#subcorpora 1\n",
    "with open(\"./dataset/researchPaper.txt\", \"r\", encoding = \"utf8\") as f:\n",
    "    corpora = f.read()\n",
    "subCorpora = nltk.word_tokenize(corpora)\n",
    "\n",
    "#top 10 most frequent words & their counts\n",
    "fdist = frequent_words(subCorpora)\n",
    "\n",
    "#words at least 10 characters long & their counts\n",
    "long_words(subCorpora, words_list, count_list)\n",
    "  \n",
    "#longest sentence with number of words\n",
    "text_file = \"researchPaper.txt\"\n",
    "longest_sentence = sentences(path, text_file)\n",
    "\n",
    "#stemmed version of the longest sentence\n",
    "stemmed_longest_sentence = stemmed_sentence(longest_sentence)\n",
    "\n",
    "#print answers\n",
    "print_answers(\"Research Paper\", subCorpora, fdist, words_list, count_list, longest_sentence, stemmed_longest_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a2a5cc",
   "metadata": {},
   "source": [
    "### subcorpus 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "831e73d3-b832-46aa-9710-e9902db95db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubCorpus: Language and Thought\n",
      "1. Length (in words): 6230 \n",
      "\n",
      "2. Lexical diversity: 0.26324237560192615 \n",
      "\n",
      "3. Top 10 most frequent words and their counts:\n",
      " [('the', 184), ('of', 173), ('to', 147), ('and', 114), ('language', 94), ('a', 88), ('in', 87), ('that', 78), ('is', 76), ('as', 63)] \n",
      "\n",
      "4. Words that are at least 10 characters long and their counts:\n",
      "accumulating 1\n",
      "acquisition 3\n",
      "adaptations 1\n",
      "adjustments 1\n",
      "adolescents 1\n",
      "alternatively 0\n",
      "application 1\n",
      "appropriate 4\n",
      "approximately 1\n",
      "automatically 4\n",
      "backgrounds 1\n",
      "behaviourists 1\n",
      "broekhuizen 0\n",
      "characteristics 1\n",
      "characterize 1\n",
      "characterized 1\n",
      "characterizes 1\n",
      "cognitively 1\n",
      "communicate 11\n",
      "communicated 1\n",
      "communicates 1\n",
      "communicating 2\n",
      "communication 17\n",
      "communicative 5\n",
      "communities 1\n",
      "complexities 1\n",
      "complicated 1\n",
      "comprehension 6\n",
      "concentrated 1\n",
      "concentrating 1\n",
      "conditioning 2\n",
      "congratulations 1\n",
      "connections 1\n",
      "connotations 1\n",
      "constructed 2\n",
      "conventional 1\n",
      "conventions 1\n",
      "conversation 1\n",
      "demonstrated 1\n",
      "development 4\n",
      "developmental 1\n",
      "differences 5\n",
      "differently 2\n",
      "disadvantage 1\n",
      "disadvantaged 1\n",
      "disorienting 1\n",
      "distinction 1\n",
      "distinguish 6\n",
      "dramatically 2\n",
      "environment 1\n",
      "environments 2\n",
      "essentially 1\n",
      "evolutionary 1\n",
      "exaggerated 3\n",
      "exaggeration 1\n",
      "exclusively 1\n",
      "experienced 1\n",
      "experiences 1\n",
      "expressions 1\n",
      "extensively 1\n",
      "facilitates 2\n",
      "fascinating 1\n",
      "generations 2\n",
      "globalwebindex 0\n",
      "grammatical 1\n",
      "handshaking 1\n",
      "historically 1\n",
      "huttenlocher 0\n",
      "impediments 1\n",
      "inappropriate 1\n",
      "information 7\n",
      "instruction 1\n",
      "instructors 1\n",
      "instrumental 2\n",
      "interacting 1\n",
      "interaction 8\n",
      "interactions 2\n",
      "interactive 1\n",
      "interesting 1\n",
      "interpreted 2\n",
      "interpreting 1\n",
      "interpretive 1\n",
      "intervention 2\n",
      "intonations 1\n",
      "investigated 1\n",
      "maintaining 1\n",
      "mathematical 1\n",
      "meaningless 2\n",
      "minimalistic 1\n",
      "mispronounced 1\n",
      "necessarily 1\n",
      "nevertheless 1\n",
      "observational 1\n",
      "occasionally 1\n",
      "opportunity 1\n",
      "overcompensate 1\n",
      "particularly 1\n",
      "practically 0\n",
      "predisposition 1\n",
      "progressive 1\n",
      "prohibitive 1\n",
      "psychologists 1\n",
      "recognizable 1\n",
      "relationship 1\n",
      "relationships 1\n",
      "remembering 1\n",
      "repetitively 1\n",
      "researchers 1\n",
      "sociocultural 1\n",
      "sophisticated 1\n",
      "sophistication 1\n",
      "spontaneously 1\n",
      "storytelling 1\n",
      "susceptible 2\n",
      "systematically 1\n",
      "telegraphic 2\n",
      "threatening 1\n",
      "toddlerhood 1\n",
      "understandably 1\n",
      "understanding 7\n",
      "understands 1\n",
      "undoubtedly 1\n",
      "universities 1\n",
      "unwittingly 1\n",
      "variability 0\n",
      "vocabularies 3\n",
      "vocalization 1\n",
      "vocalizations 7\n",
      "vouloumanos 0\n",
      "\n",
      "\n",
      "The longest sentence:\n",
      " ['Almost', 'all', ',', '97', 'percent', ',', 'of', 'Canadians', '18', 'to', '24', 'years', 'of', 'age', '(', 'sometimes', 'referred', 'to', 'as', 'Gen', 'Z', 'or', 'as', 'iGen', ')', 'own', 'a', 'smart', 'phone', 'and', 'are', 'much', 'more', 'likely', 'to', 'use', 'their', 'smart', 'phones', 'as', 'their', 'primary', 'device', ',', 'compared', 'with', 'Canadians', 'in', 'other', 'age', 'groups', ',', 'for', 'activities', 'ranging', 'from', 'reviewing', 'emails', 'to', 'accessing', 'social', 'media', 'or', 'getting', 'directions', ',', 'even', 'when', 'at', 'home', '(', 'Catalyst', ',', '2015', ';', 'GlobalWebIndex', ',', '2019', ').']\n",
      "The length of the longest sentence: 79 \n",
      "\n",
      "The stemmed longest sentence:\n",
      " ['almost', 'all', ',', '97', 'percent', ',', 'of', 'canadian', '18', 'to', '24', 'year', 'of', 'age', '(', 'sometim', 'refer', 'to', 'as', 'gen', 'z', 'or', 'as', 'igen', ')', 'own', 'a', 'smart', 'phone', 'and', 'are', 'much', 'more', 'like', 'to', 'use', 'their', 'smart', 'phone', 'as', 'their', 'primari', 'devic', ',', 'compar', 'with', 'canadian', 'in', 'other', 'age', 'group', ',', 'for', 'activ', 'rang', 'from', 'review', 'email', 'to', 'access', 'social', 'media', 'or', 'get', 'direct', ',', 'even', 'when', 'at', 'home', '(', 'catalyst', ',', '2015', ';', 'globalwebindex', ',', '2019', ').']\n",
      "The length of the stemmed longest sentence: 79 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#subcorpora 2\n",
    "with open(\"./dataset/languageAndThought.txt\", \"r\", encoding = \"utf8\") as f:\n",
    "    corpora = f.read()\n",
    "subCorpora = nltk.word_tokenize(corpora)\n",
    "\n",
    "#top 10 most frequent words & their counts\n",
    "fdist = frequent_words(subCorpora)\n",
    "\n",
    "#words at least 10 characters long & their counts\n",
    "long_words(subCorpora, words_list, count_list)\n",
    "  \n",
    "#longest sentence with number of words\n",
    "text_file = \"languageAndThought.txt\"\n",
    "longest_sentence = sentences(path, text_file)\n",
    "\n",
    "#stemmed version of the longest sentence\n",
    "stemmed_longest_sentence = stemmed_sentence(longest_sentence)\n",
    "\n",
    "#print answers\n",
    "print_answers(\"Language and Thought\", subCorpora, fdist, words_list, count_list, longest_sentence, stemmed_longest_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f9b7b7",
   "metadata": {},
   "source": [
    "### subcorpus 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3e1aa3f5-72b6-45cc-b07e-db466e44fe3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubCorpus: emails\n",
      "1. Length (in words): 7151 \n",
      "\n",
      "2. Lexical diversity: 0.20109075653754718 \n",
      "\n",
      "3. Top 10 most frequent words and their counts:\n",
      " [('I', 302), ('a', 175), ('to', 160), ('you', 159), ('the', 122), ('and', 107), ('it', 106), ('for', 78), ('m', 75), ('s', 69)] \n",
      "\n",
      "4. Words that are at least 10 characters long and their counts:\n",
      "accommodations 1\n",
      "backstories 1\n",
      "brightening 1\n",
      "challenging 1\n",
      "collectibles 1\n",
      "comfortable 1\n",
      "considering 1\n",
      "conversations 2\n",
      "daydreaming 1\n",
      "destinations 1\n",
      "distractions 1\n",
      "distributions 1\n",
      "experimenting 1\n",
      "highlighter 1\n",
      "inspiration 1\n",
      "merchandise 1\n",
      "moisturizer 1\n",
      "overwhelming 3\n",
      "photography 1\n",
      "probability 1\n",
      "professional 1\n",
      "recommendations 7\n",
      "recommended 3\n",
      "suggestions 4\n",
      "surprisingly 1\n",
      "thanksgiving 0\n",
      "therapeutic 1\n",
      "\n",
      "\n",
      "The longest sentence:\n",
      " ['I', '’', 'm', 'not', 'sure', 'if', 'I', 'want', 'to', 'go', 'anywhere', 'this', 'year', 'or', 'just', 'stay', 'local', ',', 'but', 'I', 'thought', 'I', '’', 'd', 'ask', 'if', 'you', 'had', 'any', 'ideas', 'or', 'plans', 'for', 'the', 'holidays', '.']\n",
      "The length of the longest sentence: 36 \n",
      "\n",
      "The stemmed longest sentence:\n",
      " ['i', '’', 'm', 'not', 'sure', 'if', 'i', 'want', 'to', 'go', 'anywher', 'thi', 'year', 'or', 'just', 'stay', 'local', ',', 'but', 'i', 'thought', 'i', '’', 'd', 'ask', 'if', 'you', 'had', 'ani', 'idea', 'or', 'plan', 'for', 'the', 'holiday', '.']\n",
      "The length of the stemmed longest sentence: 36 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#subcorpora 2\n",
    "with open(\"\"\"./dataset/emails.txt\"\"\", \"r\", encoding=\"utf8\") as f:\n",
    "    corpora = f.read()\n",
    "subCorpora = nltk.word_tokenize(corpora)\n",
    "\n",
    "#top 10 most frequent words & their counts\n",
    "fdist = frequent_words(subCorpora)\n",
    "\n",
    "#words at least 10 characters long & their counts\n",
    "long_words(subCorpora, words_list, count_list)\n",
    "  \n",
    "#longest sentence with number of words\n",
    "text_file = \"\"\"emails.txt\"\"\"\n",
    "longest_sentence = sentences(path, text_file)\n",
    "\n",
    "#stemmed version of the longest sentence\n",
    "stemmed_longest_sentence = stemmed_sentence(longest_sentence)\n",
    "\n",
    "#print answers\n",
    "print_answers(\"emails\", subCorpora, fdist, words_list, count_list, longest_sentence, stemmed_longest_sentence)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
